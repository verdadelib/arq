#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v2.0 - Enhanced Analysis Engine
Motor de an√°lise avan√ßado com m√∫ltiplas IAs e sistemas integrados
"""

import os
import logging
import time
import json
from datetime import datetime
from typing import Dict, List, Optional, Any
from services.gemini_client import gemini_client
from services.websailor_integration import websailor_agent
from services.deep_search_service import deep_search_service

logger = logging.getLogger(__name__)

class EnhancedAnalysisEngine:
    """Motor de an√°lise avan√ßado com integra√ß√£o de m√∫ltiplos sistemas"""
    
    def __init__(self):
        """Inicializa o motor de an√°lise"""
        self.max_analysis_time = 1800  # 30 minutos
        self.systems_enabled = {
            'gemini': bool(gemini_client),
            'websailor': websailor_agent.is_available(),
            'deep_search': bool(deep_search_service)
        }
        
        logger.info(f"Enhanced Analysis Engine inicializado - Sistemas: {self.systems_enabled}")
    
    def generate_comprehensive_analysis(
        self, 
        data: Dict[str, Any],
        session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Gera an√°lise abrangente usando todos os sistemas dispon√≠veis"""
        
        start_time = time.time()
        logger.info(f"üöÄ Iniciando an√°lise abrangente para {data.get('segmento')}")
        
        try:
            # FASE 1: Coleta de dados
            logger.info("üìä FASE 1: Coleta de dados...")
            research_data = self._collect_research_data(data, session_id)
            
            # FASE 2: An√°lise com IA
            logger.info("üß† FASE 2: An√°lise com IA...")
            ai_analysis = self._perform_ai_analysis(data, research_data)
            
            # FASE 3: Consolida√ß√£o final
            logger.info("üéØ FASE 3: Consolida√ß√£o final...")
            final_analysis = self._consolidate_analysis(data, research_data, ai_analysis)
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            # Adiciona metadados
            final_analysis["metadata"] = {
                "processing_time_seconds": processing_time,
                "processing_time_formatted": f"{int(processing_time // 60)}m {int(processing_time % 60)}s",
                "analysis_engine": "ARQV30 Enhanced v2.0",
                "systems_used": [k for k, v in self.systems_enabled.items() if v],
                "generated_at": datetime.utcnow().isoformat(),
                "quality_score": self._calculate_quality_score(final_analysis),
                "data_sources_used": len(research_data.get("sources", [])),
                "ai_models_used": 1 if self.systems_enabled['gemini'] else 0
            }
            
            logger.info(f"‚úÖ An√°lise abrangente conclu√≠da em {processing_time:.2f} segundos")
            return final_analysis
            
        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise abrangente: {str(e)}", exc_info=True)
            return self._generate_fallback_analysis(data, str(e))
    
    def _collect_research_data(
        self, 
        data: Dict[str, Any], 
        session_id: Optional[str]
    ) -> Dict[str, Any]:
        """Coleta dados de pesquisa de m√∫ltiplas fontes"""
        
        research_data = {
            "web_research": {},
            "deep_search": {},
            "sources": [],
            "total_content_length": 0
        }
        
        # 1. Pesquisa web com WebSailor
        if self.systems_enabled['websailor'] and data.get('query'):
            logger.info("üåê Executando pesquisa web com WebSailor...")
            try:
                web_result = websailor_agent.navigate_and_research(
                    data['query'],
                    context={
                        "segmento": data.get('segmento'),
                        "produto": data.get('produto'),
                        "publico": data.get('publico')
                    },
                    max_pages=8,
                    depth=2,
                    aggressive_mode=True
                )
                research_data["web_research"] = web_result
                research_data["sources"].extend(web_result.get("sources", []))
                
                # Adiciona conte√∫do combinado
                combined_content = web_result.get("research_summary", {}).get("combined_content", "")
                research_data["total_content_length"] += len(combined_content)
                
                logger.info(f"‚úÖ WebSailor: {len(web_result.get('sources', []))} fontes analisadas")
            except Exception as e:
                logger.error(f"Erro no WebSailor: {str(e)}")
        
        # 2. Deep Search
        if self.systems_enabled['deep_search'] and data.get('query'):
            logger.info("üî¨ Executando deep search...")
            try:
                deep_result = deep_search_service.perform_deep_search(
                    data['query'],
                    data,
                    max_results=15
                )
                research_data["deep_search"] = deep_result
                research_data["total_content_length"] += len(str(deep_result))
                
                logger.info("‚úÖ Deep search conclu√≠do")
            except Exception as e:
                logger.error(f"Erro no Deep Search: {str(e)}")
        
        return research_data
    
    def _perform_ai_analysis(
        self, 
        data: Dict[str, Any], 
        research_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Executa an√°lise com IA usando Gemini"""
        
        if not self.systems_enabled['gemini']:
            logger.warning("Gemini n√£o dispon√≠vel - usando an√°lise b√°sica")
            return self._generate_basic_analysis(data)
        
        try:
            # Prepara contexto de pesquisa
            search_context = ""
            if research_data.get("web_research"):
                web_summary = research_data["web_research"].get("research_summary", {})
                search_context += f"PESQUISA WEB:\n{web_summary.get('combined_content', '')}\n\n"
                
                insights = web_summary.get("key_insights", [])
                if insights:
                    search_context += f"INSIGHTS WEB:\n" + "\n".join(insights) + "\n\n"
            
            if research_data.get("deep_search"):
                search_context += f"DEEP SEARCH:\n{research_data['deep_search']}\n\n"
            
            # Executa an√°lise com Gemini
            logger.info("ü§ñ Executando an√°lise com Gemini Pro...")
            gemini_result = gemini_client.generate_ultra_detailed_analysis(
                data,
                search_context=search_context[:15000] if search_context else None,
                attachments_context=None
            )
            
            logger.info("‚úÖ An√°lise Gemini conclu√≠da")
            return gemini_result
            
        except Exception as e:
            logger.error(f"Erro na an√°lise Gemini: {str(e)}")
            return self._generate_basic_analysis(data)
    
    def _consolidate_analysis(
        self, 
        data: Dict[str, Any], 
        research_data: Dict[str, Any], 
        ai_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Consolida toda a an√°lise"""
        
        # Usa an√°lise da IA como base
        consolidated = ai_analysis.copy()
        
        # Enriquece com dados de pesquisa
        if research_data.get("web_research"):
            consolidated["pesquisa_web_detalhada"] = research_data["web_research"]
        
        if research_data.get("deep_search"):
            consolidated["pesquisa_profunda"] = research_data["deep_search"]
        
        # Adiciona insights exclusivos baseados na pesquisa
        exclusive_insights = self._generate_exclusive_insights(data, research_data, ai_analysis)
        if exclusive_insights:
            existing_insights = consolidated.get("insights_exclusivos", [])
            consolidated["insights_exclusivos"] = existing_insights + exclusive_insights
        
        return consolidated
    
    def _generate_exclusive_insights(
        self, 
        data: Dict[str, Any], 
        research_data: Dict[str, Any], 
        ai_analysis: Dict[str, Any]
    ) -> List[str]:
        """Gera insights exclusivos baseados na pesquisa"""
        
        insights = []
        
        # Insights baseados na pesquisa web
        if research_data.get("web_research"):
            web_insights = research_data["web_research"].get("research_summary", {}).get("key_insights", [])
            for insight in web_insights[:3]:
                insights.append(f"üåê Pesquisa Web: {insight}")
        
        # Insights baseados no deep search
        if research_data.get("deep_search"):
            insights.append(f"üîç Deep Search: An√°lise profunda realizada com {len(research_data.get('sources', []))} fontes")
        
        # Insights sobre qualidade dos dados
        total_sources = len(research_data.get("sources", []))
        if total_sources > 0:
            insights.append(f"üìä Qualidade dos Dados: An√°lise baseada em {total_sources} fontes verificadas")
        
        return insights[:5]  # M√°ximo 5 insights exclusivos
    
    def _generate_basic_analysis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Gera an√°lise b√°sica quando IA n√£o est√° dispon√≠vel"""
        
        return {
            "avatar_ultra_detalhado": {
                "perfil_demografico": {
                    "idade": "25-45 anos",
                    "renda": "R$ 3.000 - R$ 15.000",
                    "escolaridade": "Superior",
                    "localizacao": "Centros urbanos"
                },
                "dores_especificas": [
                    "Falta de conhecimento especializado",
                    "Dificuldade para implementar estrat√©gias",
                    "Resultados inconsistentes",
                    "Falta de direcionamento claro"
                ],
                "desejos_profundos": [
                    "Alcan√ßar liberdade financeira",
                    "Ter mais tempo para fam√≠lia",
                    "Ser reconhecido como especialista",
                    "Fazer diferen√ßa no mundo"
                ]
            },
            "escopo": {
                "posicionamento_mercado": "Solu√ß√£o premium para resultados r√°pidos",
                "proposta_valor": "Transforme seu neg√≥cio com estrat√©gias comprovadas",
                "diferenciais_competitivos": ["Metodologia exclusiva", "Suporte personalizado"]
            },
            "estrategia_palavras_chave": {
                "palavras_primarias": [data.get('segmento', 'neg√≥cio'), "estrat√©gia", "marketing"],
                "palavras_secundarias": ["crescimento", "vendas", "digital", "online"],
                "palavras_cauda_longa": [f"como crescer no {data.get('segmento', 'mercado')}", "estrat√©gias de marketing digital"]
            },
            "insights_exclusivos": [
                f"O mercado de {data.get('segmento', 'neg√≥cios')} apresenta oportunidades de crescimento",
                "A digitaliza√ß√£o √© uma tend√™ncia irrevers√≠vel no setor",
                "Investimento em marketing digital √© essencial para competitividade",
                "Personaliza√ß√£o da experi√™ncia do cliente √© um diferencial competitivo",
                "An√°lise gerada em modo b√°sico - configure as APIs para an√°lise completa"
            ]
        }
    
    def _calculate_quality_score(self, analysis: Dict[str, Any]) -> float:
        """Calcula score de qualidade da an√°lise"""
        
        score = 0.0
        max_score = 100.0
        
        # Pontua√ß√£o por se√ß√µes principais (60 pontos)
        main_sections = [
            "avatar_ultra_detalhado", "escopo", "estrategia_palavras_chave", "insights_exclusivos"
        ]
        
        for section in main_sections:
            if section in analysis and analysis[section]:
                score += 15.0  # 60/4 = 15 pontos por se√ß√£o
        
        # Pontua√ß√£o por pesquisa (20 pontos)
        if "pesquisa_web_detalhada" in analysis:
            score += 10.0
        if "pesquisa_profunda" in analysis:
            score += 10.0
        
        # Pontua√ß√£o por insights (20 pontos)
        insights = analysis.get("insights_exclusivos", [])
        if len(insights) >= 5:
            score += 20.0
        elif len(insights) >= 3:
            score += 15.0
        elif len(insights) >= 1:
            score += 10.0
        
        return min(score, max_score)
    
    def _generate_fallback_analysis(self, data: Dict[str, Any], error: str) -> Dict[str, Any]:
        """Gera an√°lise de emerg√™ncia"""
        
        logger.error(f"Gerando an√°lise de emerg√™ncia devido a: {error}")
        
        basic_analysis = self._generate_basic_analysis(data)
        basic_analysis["insights_exclusivos"].append(f"‚ö†Ô∏è Erro no processamento: {error}")
        basic_analysis["insights_exclusivos"].append("üîÑ Recomenda-se executar nova an√°lise")
        
        basic_analysis["metadata"] = {
            "processing_time_seconds": 0,
            "analysis_engine": "Emergency Fallback",
            "generated_at": datetime.utcnow().isoformat(),
            "quality_score": 25.0,
            "error": error,
            "recommendation": "Execute nova an√°lise com configura√ß√£o completa"
        }
        
        return basic_analysis

# Inst√¢ncia global do motor
enhanced_analysis_engine = EnhancedAnalysisEngine()