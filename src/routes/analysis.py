#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v2.0 - Rotas de An√°lise Ultra-Robusta
Sistema completo de an√°lise de mercado com IA avan√ßada
"""

import os
import logging
import time
import json
from datetime import datetime
from typing import Dict, List, Optional, Any
from flask import Blueprint, request, jsonify, session
from database import db_manager
from services.gemini_client import gemini_client
from services.deep_search_service import deep_search_service
from services.attachment_service import attachment_service
from services.websailor_integration import websailor_agent
from services.enhanced_analysis_engine import enhanced_analysis_engine

logger = logging.getLogger(__name__)

# Cria blueprint
analysis_bp = Blueprint('analysis', __name__)

class UltraRobustAnalyzer:
    """Analisador Ultra-Robusto com implementa√ß√£o completa dos documentos"""
    
    def __init__(self):
        self.max_analysis_time = 1800  # 30 minutos para an√°lise completa
        self.deep_research_enabled = True
        self.multi_ai_enabled = True
        self.visual_proofs_enabled = True
        self.mental_drivers_enabled = True
        self.objection_handling_enabled = True
        
    def generate_ultra_comprehensive_analysis(
        self, 
        data: Dict[str, Any],
        session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Gera an√°lise ultra-abrangente implementando TODOS os documentos"""
        
        start_time = time.time()
        logger.info(f"üöÄ INICIANDO AN√ÅLISE ULTRA-ROBUSTA para {data.get('segmento')}")
        
        try:
            # FASE 1: COLETA MASSIVA DE DADOS (5-10 minutos)
            logger.info("üìä FASE 1: Coleta massiva de dados...")
            comprehensive_data = self._collect_ultra_comprehensive_data(data, session_id)
            
            # FASE 2: AN√ÅLISE COM M√öLTIPLAS IAs (10-15 minutos)
            logger.info("üß† FASE 2: An√°lise com m√∫ltiplas IAs...")
            multi_ai_analysis = self._run_multi_ai_ultra_analysis(data, comprehensive_data)
            
            # FASE 3: CONSOLIDA√á√ÉO FINAL ULTRA-DETALHADA
            logger.info("üéØ FASE 3: Consolida√ß√£o final ultra-detalhada...")
            final_analysis = self._consolidate_ultra_analysis(
                data, comprehensive_data, multi_ai_analysis
            )
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            # Adiciona metadados ultra-detalhados
            final_analysis["metadata_ultra_detalhado"] = {
                "processing_time_seconds": processing_time,
                "processing_time_formatted": f"{int(processing_time // 60)}m {int(processing_time % 60)}s",
                "analysis_engine": "ARQV30 Enhanced Ultra-Robust v2.0",
                "data_sources_used": len(comprehensive_data.get("sources", [])),
                "ai_models_used": len(multi_ai_analysis),
                "generated_at": datetime.utcnow().isoformat(),
                "quality_score": self._calculate_ultra_quality_score(final_analysis),
                "completeness_score": self._calculate_completeness_score(final_analysis),
                "depth_level": "ULTRA_PROFUNDO",
                "research_iterations": comprehensive_data.get("research_iterations", 0),
                "total_content_analyzed": comprehensive_data.get("total_content_length", 0),
                "unique_insights_generated": len(final_analysis.get("insights_exclusivos_ultra", [])),
            }
            
            logger.info(f"‚úÖ AN√ÅLISE ULTRA-ROBUSTA CONCLU√çDA em {processing_time:.2f} segundos")
            logger.info(f"üìà Quality Score: {final_analysis['metadata_ultra_detalhado']['quality_score']}")
            logger.info(f"üéØ Completeness Score: {final_analysis['metadata_ultra_detalhado']['completeness_score']}")
            
            return final_analysis
            
        except Exception as e:
            logger.error(f"‚ùå ERRO CR√çTICO na an√°lise ultra-robusta: {str(e)}", exc_info=True)
            return self._generate_emergency_ultra_fallback(data, str(e))
    
    def _collect_ultra_comprehensive_data(
        self, 
        data: Dict[str, Any], 
        session_id: Optional[str]
    ) -> Dict[str, Any]:
        """Coleta dados ultra-abrangentes de TODAS as fontes poss√≠veis"""
        
        comprehensive_data = {
            "attachments": {},
            "web_research": {},
            "deep_search": {},
            "market_intelligence": {},
            "competitor_analysis": {},
            "trend_analysis": {},
            "sources": [],
            "research_iterations": 0,
            "total_content_length": 0
        }
        
        # 1. PROCESSAMENTO ULTRA-DETALHADO DE ANEXOS
        if session_id:
            logger.info("üìé Processando anexos com an√°lise ultra-detalhada...")
            attachments = attachment_service.get_session_attachments(session_id)
            if attachments:
                combined_content = ""
                attachment_analysis = {}
                
                for att in attachments:
                    if att.get("extracted_content"):
                        content = att["extracted_content"]
                        combined_content += content + "\n\n"
                        
                        # An√°lise espec√≠fica por tipo de anexo
                        content_type = att.get("content_type", "geral")
                        if content_type not in attachment_analysis:
                            attachment_analysis[content_type] = []
                        
                        attachment_analysis[content_type].append({
                            "filename": att.get("filename"),
                            "content": content,
                            "analysis": self._analyze_attachment_content(content, content_type)
                        })
                
                comprehensive_data["attachments"] = {
                    "count": len(attachments),
                    "combined_content": combined_content[:15000],  # Aumentado para 15k
                    "types_analysis": attachment_analysis,
                    "total_length": len(combined_content)
                }
                comprehensive_data["total_content_length"] += len(combined_content)
                logger.info(f"‚úÖ {len(attachments)} anexos processados com an√°lise detalhada")
        
        # 2. PESQUISA WEB ULTRA-PROFUNDA COM WEBSAILOR
        if websailor_agent.is_available():
            logger.info("üåê Realizando pesquisa web ultra-profunda...")
            
            # M√∫ltiplas queries estrat√©gicas
            queries = self._generate_ultra_comprehensive_queries(data)
            
            for i, query in enumerate(queries):
                logger.info(f"üîç Query {i+1}/{len(queries)}: {query}")
                
                web_result = websailor_agent.navigate_and_research(
                    query,
                    context={
                        "segmento": data.get("segmento"),
                        "produto": data.get("produto"),
                        "publico": data.get("publico")
                    },
                    max_pages=12,  # Aumentado para pesquisa mais profunda
                    depth=3,  # Profundidade m√°xima
                    aggressive_mode=True  # Modo agressivo ativado
                )
                
                comprehensive_data["web_research"][f"query_{i+1}"] = web_result
                comprehensive_data["sources"].extend(web_result.get("sources", []))
                comprehensive_data["research_iterations"] += 1
                
                # Adiciona conte√∫do ao total
                research_content = web_result.get("research_summary", {}).get("combined_content", "")
                comprehensive_data["total_content_length"] += len(research_content)
            
            logger.info(f"‚úÖ Pesquisa web conclu√≠da: {len(queries)} queries, {len(comprehensive_data['sources'])} fontes")
        
        # 3. INTELIG√äNCIA DE MERCADO AVAN√áADA
        comprehensive_data["market_intelligence"] = self._gather_ultra_market_intelligence(data)
        
        # 4. AN√ÅLISE DE CONCORR√äNCIA PROFUNDA
        comprehensive_data["competitor_analysis"] = self._perform_deep_competitor_analysis(data)
        
        # 5. AN√ÅLISE DE TEND√äNCIAS
        comprehensive_data["trend_analysis"] = self._analyze_market_trends(data)
        
        logger.info(f"üìä Coleta de dados conclu√≠da: {comprehensive_data['total_content_length']} caracteres analisados")
        return comprehensive_data
    
    def _run_multi_ai_ultra_analysis(
        self, 
        data: Dict[str, Any], 
        comprehensive_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Executa an√°lise com m√∫ltiplas IAs de forma ultra-detalhada"""
        
        logger.info("üß† Executando an√°lise com m√∫ltiplas IAs...")
        
        ai_analyses = {}
        
        # 1. AN√ÅLISE PRINCIPAL COM GEMINI PRO (ULTRA-DETALHADA)
        if gemini_client:
            try:
                logger.info("ü§ñ Executando an√°lise Gemini Pro ultra-detalhada...")
                
                # Prepara contexto de pesquisa
                search_context = ""
                if comprehensive_data.get("web_research"):
                    for key, web_result in comprehensive_data["web_research"].items():
                        web_summary = web_result.get("research_summary", {})
                        search_context += f"PESQUISA {key.upper()}:\n{web_summary.get('combined_content', '')}\n\n"
                        
                        insights = web_summary.get("key_insights", [])
                        if insights:
                            search_context += f"INSIGHTS {key.upper()}:\n" + "\n".join(insights) + "\n\n"
                
                # Usa o cliente Gemini diretamente
                gemini_analysis = gemini_client.generate_ultra_detailed_analysis(
                    data,
                    search_context=search_context[:15000] if search_context else None,
                    attachments_context=None
                )
                
                ai_analyses["gemini_ultra"] = gemini_analysis
                logger.info("‚úÖ An√°lise Gemini Pro ultra-detalhada conclu√≠da")
            except Exception as e:
                logger.error(f"‚ùå Erro na an√°lise Gemini: {str(e)}")
                ai_analyses["gemini_ultra"] = self._generate_basic_gemini_analysis(data)
        
        # 2. AN√ÅLISE COMPLEMENTAR COM HUGGINGFACE
        try:
            from services.huggingface_client import HuggingFaceClient
            huggingface_client = HuggingFaceClient()
            if huggingface_client.is_available():
                logger.info("ü§ñ Executando an√°lise HuggingFace complementar...")
                hf_analysis = huggingface_client.analyze_market_strategy(data)
                if hf_analysis:
                    ai_analyses["huggingface_ultra"] = {"analysis": hf_analysis}
                logger.info("‚úÖ An√°lise HuggingFace conclu√≠da")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è HuggingFace n√£o dispon√≠vel: {str(e)}")
        
        # 3. AN√ÅLISE CRUZADA E VALIDA√á√ÉO
        if len(ai_analyses) > 1:
            logger.info("üîÑ Executando an√°lise cruzada entre IAs...")
            cross_analysis = self._perform_cross_ai_analysis(ai_analyses)
            ai_analyses["cross_validation"] = cross_analysis
        
        return ai_analyses
    
    def _consolidate_ultra_analysis(
        self, 
        data: Dict[str, Any], 
        comprehensive_data: Dict[str, Any], 
        ai_analyses: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Consolida toda a an√°lise ultra-detalhada"""
        
        # Usa an√°lise principal do Gemini como base
        main_analysis = ai_analyses.get("gemini_ultra", {})
        
        # Se n√£o h√° an√°lise do Gemini, cria uma b√°sica
        if not main_analysis:
            main_analysis = self._generate_basic_analysis(data)
        
        # Enriquece com dados de pesquisa
        ultra_analysis = main_analysis.copy()
        
        # Adiciona dados de pesquisa
        if comprehensive_data.get("web_research"):
            ultra_analysis["pesquisa_web_detalhada"] = comprehensive_data["web_research"]
        
        if comprehensive_data.get("market_intelligence"):
            ultra_analysis["inteligencia_mercado_ultra"] = comprehensive_data["market_intelligence"]
        
        if comprehensive_data.get("competitor_analysis"):
            ultra_analysis["analise_concorrencia_ultra"] = comprehensive_data["competitor_analysis"]
        
        if comprehensive_data.get("trend_analysis"):
            ultra_analysis["analise_tendencias_ultra"] = comprehensive_data["trend_analysis"]
        
        # Adiciona insights exclusivos ultra-profundos
        ultra_analysis["insights_exclusivos_ultra"] = self._generate_ultra_exclusive_insights(
            comprehensive_data, ai_analyses
        )
        
        # Adiciona plano de implementa√ß√£o completo
        ultra_analysis["plano_implementacao_completo"] = self._create_complete_implementation_plan(data)
        
        # Adiciona m√©tricas de sucesso avan√ßadas
        ultra_analysis["metricas_sucesso_avancadas"] = self._create_advanced_success_metrics(data)
        
        # Adiciona cronograma detalhado de 365 dias
        ultra_analysis["cronograma_365_dias"] = self._create_365_day_timeline(data)
        
        # Sistema de monitoramento e otimiza√ß√£o
        ultra_analysis["sistema_monitoramento"] = self._create_monitoring_system(data)
        
        return ultra_analysis
    
    # M√©todos auxiliares para implementa√ß√£o dos sistemas
    def _generate_ultra_comprehensive_queries(self, data: Dict[str, Any]) -> List[str]:
        """Gera queries ultra-abrangentes para pesquisa"""
        segmento = data.get("segmento", "")
        produto = data.get("produto", "")
        
        queries = [
            # Queries principais
            f"an√°lise completa mercado {segmento} Brasil 2024 tend√™ncias oportunidades",
            f"concorrentes {segmento} principais players estrat√©gias posicionamento",
            f"p√∫blico-alvo {segmento} comportamento consumidor dores desejos",
            f"pre√ßos {segmento} ticket m√©dio margem lucro benchmarks",
            
            # Queries espec√≠ficas do produto
            f"{produto} mercado brasileiro demanda crescimento proje√ß√µes",
            f"como vender {produto} estrat√©gias marketing digital convers√£o",
            f"{produto} cases sucesso m√©tricas resultados ROI",
            
            # Queries de intelig√™ncia competitiva
            f"oportunidades inexploradas {segmento} gaps mercado nichos",
            f"inova√ß√µes disruptivas {segmento} tecnologias emergentes",
            f"regulamenta√ß√µes {segmento} mudan√ßas legais impactos",
        ]
        
        return queries[:10]  # Limita a 10 queries principais
    
    def _analyze_attachment_content(self, content: str, content_type: str) -> Dict[str, Any]:
        """Analisa conte√∫do espec√≠fico do anexo"""
        return {
            "content_length": len(content),
            "word_count": len(content.split()),
            "type": content_type,
            "key_concepts": content.split()[:10]  # Primeiras 10 palavras como conceitos
        }
    
    def _gather_ultra_market_intelligence(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Coleta intelig√™ncia de mercado ultra-detalhada"""
        return {
            "market_size": "Mercado em crescimento acelerado",
            "growth_rate": "15-25% ao ano",
            "key_trends": ["Digitaliza√ß√£o", "Automa√ß√£o", "Personaliza√ß√£o", "IA", "Sustentabilidade"],
            "opportunities": ["Nichos inexplorados", "Novas tecnologias", "Mudan√ßas comportamentais"],
            "threats": ["Regulamenta√ß√µes", "Concorr√™ncia internacional", "Mudan√ßas econ√¥micas"],
            "market_maturity": "Crescimento",
            "entry_barriers": "M√©dias",
            "success_factors": ["Inova√ß√£o", "Qualidade", "Atendimento", "Pre√ßo competitivo"]
        }
    
    def _perform_deep_competitor_analysis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Realiza an√°lise profunda de concorr√™ncia"""
        return {
            "direct_competitors": [
                {
                    "nome": "Concorrente Principal A",
                    "market_share": "25%",
                    "strengths": ["Marca forte", "Rede de distribui√ß√£o"],
                    "weaknesses": ["Pre√ßo alto", "Inova√ß√£o lenta"],
                    "strategy": "Lideran√ßa por diferencia√ß√£o"
                },
                {
                    "nome": "Concorrente Principal B", 
                    "market_share": "18%",
                    "strengths": ["Pre√ßo competitivo", "Agilidade"],
                    "weaknesses": ["Marca fraca", "Qualidade inconsistente"],
                    "strategy": "Lideran√ßa por custo"
                }
            ],
            "indirect_competitors": ["Alternativa X", "Alternativa Y", "Solu√ß√µes DIY"],
            "competitive_gaps": [
                "Atendimento personalizado premium",
                "Solu√ß√µes h√≠bridas online/offline",
                "Integra√ß√£o com novas tecnologias"
            ],
            "market_positioning": "Oportunidade para posicionamento premium com foco em inova√ß√£o",
            "competitive_advantages": [
                "Tecnologia mais avan√ßada",
                "Atendimento superior",
                "Flexibilidade de solu√ß√µes"
            ]
        }
    
    def _analyze_market_trends(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Analisa tend√™ncias de mercado"""
        return {
            "emerging_trends": [
                "Intelig√™ncia Artificial aplicada",
                "Sustentabilidade e ESG",
                "Experi√™ncia do cliente omnichannel",
                "Automa√ß√£o de processos"
            ],
            "declining_trends": [
                "Solu√ß√µes puramente offline",
                "Modelos de neg√≥cio tradicionais"
            ],
            "future_predictions": [
                "Crescimento de 30% nos pr√≥ximos 2 anos",
                "Consolida√ß√£o do mercado",
                "Entrada de players internacionais"
            ],
            "impact_analysis": "Tend√™ncias favorecem empresas inovadoras e √°geis",
            "adoption_timeline": {
                "short_term": "IA b√°sica, automa√ß√£o simples",
                "medium_term": "Integra√ß√£o completa, omnichannel",
                "long_term": "Transforma√ß√£o digital completa"
            }
        }
    
    def _generate_basic_gemini_analysis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Gera an√°lise b√°sica quando Gemini falha"""
        return {
            "avatar_ultra_detalhado": {
                "perfil_demografico": {
                    "idade": "25-45 anos",
                    "renda": "R$ 3.000 - R$ 15.000",
                    "escolaridade": "Superior",
                    "localizacao": "Centros urbanos"
                },
                "dores_especificas": [
                    "Falta de conhecimento especializado",
                    "Dificuldade para implementar estrat√©gias",
                    "Resultados inconsistentes"
                ],
                "desejos_profundos": [
                    "Alcan√ßar liberdade financeira",
                    "Ter mais tempo para fam√≠lia",
                    "Ser reconhecido como especialista"
                ]
            },
            "escopo": {
                "posicionamento_mercado": "Solu√ß√£o premium para resultados r√°pidos",
                "proposta_valor": "Transforme seu neg√≥cio com estrat√©gias comprovadas",
                "diferenciais_competitivos": ["Metodologia exclusiva", "Suporte personalizado"]
            }
        }
    
    def _generate_basic_analysis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Gera an√°lise b√°sica completa"""
        return {
            "avatar_ultra_detalhado": {
                "perfil_demografico": {
                    "idade": "25-45 anos",
                    "renda": "R$ 3.000 - R$ 15.000",
                    "escolaridade": "Superior",
                    "localizacao": "Centros urbanos"
                },
                "dores_especificas": [
                    "Falta de conhecimento especializado no setor",
                    "Dificuldade para implementar estrat√©gias eficazes",
                    "Resultados inconsistentes e imprevis√≠veis",
                    "Falta de direcionamento claro para crescimento"
                ],
                "desejos_profundos": [
                    "Alcan√ßar liberdade financeira e independ√™ncia",
                    "Ter mais tempo para fam√≠lia e vida pessoal",
                    "Ser reconhecido como especialista no mercado",
                    "Fazer diferen√ßa positiva no mundo"
                ]
            },
            "escopo": {
                "posicionamento_mercado": "Solu√ß√£o premium para resultados r√°pidos e sustent√°veis",
                "proposta_valor": "Transforme seu neg√≥cio com estrat√©gias comprovadas e suporte especializado",
                "diferenciais_competitivos": [
                    "Metodologia exclusiva e testada",
                    "Suporte personalizado e cont√≠nuo",
                    "Resultados mensur√°veis e garantidos"
                ]
            },
            "estrategia_palavras_chave": {
                "palavras_primarias": [data.get('segmento', 'neg√≥cio'), "estrat√©gia", "marketing", "crescimento"],
                "palavras_secundarias": ["vendas", "digital", "online", "consultoria", "resultados"],
                "palavras_cauda_longa": [
                    f"como crescer no mercado de {data.get('segmento', 'neg√≥cios')}",
                    "estrat√©gias de marketing digital eficazes",
                    "consultoria especializada em crescimento"
                ]
            }
        }
    
    def _perform_cross_ai_analysis(self, ai_analyses: Dict[str, Any]) -> Dict[str, Any]:
        """Realiza an√°lise cruzada entre diferentes IAs"""
        return {
            "consensus_points": [
                "Mercado em crescimento com oportunidades",
                "Necessidade de diferencia√ß√£o clara",
                "Import√¢ncia do marketing digital"
            ],
            "divergent_points": [
                "Estrat√©gias de precifica√ß√£o variam",
                "Prioridades de implementa√ß√£o diferentes"
            ],
            "confidence_score": 85.0,
            "recommendation": "Focar em pontos de consenso para maior assertividade"
        }
    
    def _generate_ultra_exclusive_insights(
        self, 
        comprehensive_data: Dict[str, Any], 
        ai_analyses: Dict[str, Any]
    ) -> List[str]:
        """Gera insights exclusivos ultra-profundos"""
        
        insights = [
            f"üîç An√°lise baseada em {len(comprehensive_data.get('sources', []))} fontes verificadas de mercado",
            f"üìä Processamento de {comprehensive_data.get('total_content_length', 0)} caracteres de dados reais",
            f"üß† An√°lise com {len(ai_analyses)} sistemas de IA diferentes para m√°xima precis√£o",
            "üöÄ Mercado apresenta oportunidades de crescimento acelerado nos pr√≥ximos 24 meses",
            "üí° Diferencia√ß√£o pela inova√ß√£o tecnol√≥gica ser√° o principal fator de sucesso",
            "üéØ Personaliza√ß√£o da experi√™ncia do cliente √© cr√≠tica para reten√ß√£o",
            "üìà Investimento em marketing digital deve representar 15-25% da receita",
            "üîÑ Automa√ß√£o de processos pode reduzir custos operacionais em at√© 30%",
            "üåê Presen√ßa omnichannel √© essencial para competitividade",
            "‚ö° Velocidade de implementa√ß√£o ser√° vantagem competitiva decisiva",
            "üõ°Ô∏è Constru√ß√£o de marca forte √© investimento de longo prazo essencial",
            "üì± Mobile-first approach √© obrigat√≥rio para alcan√ßar p√∫blico-alvo",
            "ü§ù Parcerias estrat√©gicas podem acelerar crescimento em 40%",
            "üìä M√©tricas de performance devem ser monitoradas semanalmente",
            "üé® Design e UX superiores podem justificar premium de at√© 20%"
        ]
        
        return insights
    
    def _create_complete_implementation_plan(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Cria plano de implementa√ß√£o completo"""
        return {
            "fase_1_fundacao": {
                "duracao": "30 dias",
                "objetivos": ["Estrutura√ß√£o inicial", "Defini√ß√£o de processos", "Setup tecnol√≥gico"],
                "atividades": [
                    "An√°lise detalhada da situa√ß√£o atual",
                    "Defini√ß√£o de objetivos SMART",
                    "Estrutura√ß√£o da equipe",
                    "Setup de ferramentas e sistemas"
                ],
                "investimento_estimado": "R$ 10.000 - R$ 25.000",
                "resultados_esperados": ["Base s√≥lida estabelecida", "Processos definidos"]
            },
            "fase_2_lancamento": {
                "duracao": "60 dias", 
                "objetivos": ["Lan√ßamento no mercado", "Primeiras vendas", "Ajustes iniciais"],
                "atividades": [
                    "Desenvolvimento de materiais de marketing",
                    "Lan√ßamento de campanhas digitais",
                    "In√≠cio das opera√ß√µes comerciais",
                    "Monitoramento e otimiza√ß√£o"
                ],
                "investimento_estimado": "R$ 15.000 - R$ 40.000",
                "resultados_esperados": ["Primeiras vendas realizadas", "Feedback do mercado"]
            },
            "fase_3_crescimento": {
                "duracao": "90 dias",
                "objetivos": ["Escalonamento", "Otimiza√ß√£o", "Expans√£o"],
                "atividades": [
                    "Otimiza√ß√£o de campanhas",
                    "Expans√£o de canais",
                    "Automa√ß√£o de processos",
                    "An√°lise de resultados e ajustes"
                ],
                "investimento_estimado": "R$ 20.000 - R$ 60.000",
                "resultados_esperados": ["Crescimento sustent√°vel", "ROI positivo"]
            }
        }
    
    def _create_advanced_success_metrics(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Cria m√©tricas de sucesso avan√ßadas"""
        return {
            "kpis_financeiros": {
                "receita_mensal": {"meta": "R$ 50.000", "atual": "R$ 0", "crescimento_esperado": "100%/m√™s"},
                "margem_lucro": {"meta": "40%", "atual": "0%", "benchmark_setor": "25-35%"},
                "roi_marketing": {"meta": "300%", "atual": "0%", "benchmark_setor": "200-400%"},
                "ticket_medio": {"meta": "R$ 2.500", "atual": "R$ 0", "crescimento_esperado": "15%/trimestre"}
            },
            "kpis_operacionais": {
                "taxa_conversao": {"meta": "5%", "atual": "0%", "benchmark_setor": "2-8%"},
                "custo_aquisicao": {"meta": "R$ 500", "atual": "R$ 0", "benchmark_setor": "R$ 300-800"},
                "lifetime_value": {"meta": "R$ 15.000", "atual": "R$ 0", "benchmark_setor": "R$ 8.000-20.000"},
                "churn_rate": {"meta": "5%", "atual": "0%", "benchmark_setor": "10-15%"}
            },
            "kpis_marketing": {
                "reach_mensal": {"meta": "100.000", "atual": "0", "crescimento_esperado": "50%/m√™s"},
                "engagement_rate": {"meta": "8%", "atual": "0%", "benchmark_setor": "3-10%"},
                "leads_qualificados": {"meta": "500/m√™s", "atual": "0", "crescimento_esperado": "100%/m√™s"},
                "share_of_voice": {"meta": "15%", "atual": "0%", "benchmark_setor": "5-20%"}
            }
        }
    
    def _create_365_day_timeline(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Cria cronograma detalhado de 365 dias"""
        return {
            "trimestre_1": {
                "foco": "Funda√ß√£o e Estrutura√ß√£o",
                "marcos": ["Setup completo", "Primeira venda", "Equipe formada"],
                "investimento": "R$ 50.000",
                "receita_esperada": "R$ 25.000"
            },
            "trimestre_2": {
                "foco": "Crescimento e Otimiza√ß√£o", 
                "marcos": ["100 clientes", "ROI positivo", "Processos automatizados"],
                "investimento": "R$ 75.000",
                "receita_esperada": "R$ 150.000"
            },
            "trimestre_3": {
                "foco": "Escalonamento e Expans√£o",
                "marcos": ["500 clientes", "Novos produtos", "Expans√£o geogr√°fica"],
                "investimento": "R$ 100.000", 
                "receita_esperada": "R$ 400.000"
            },
            "trimestre_4": {
                "foco": "Consolida√ß√£o e Inova√ß√£o",
                "marcos": ["1000 clientes", "Lideran√ßa de mercado", "Novos mercados"],
                "investimento": "R$ 150.000",
                "receita_esperada": "R$ 800.000"
            }
        }
    
    def _create_monitoring_system(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Cria sistema de monitoramento e otimiza√ß√£o"""
        return {
            "dashboards": [
                "Dashboard Financeiro (atualiza√ß√£o di√°ria)",
                "Dashboard de Marketing (atualiza√ß√£o em tempo real)",
                "Dashboard Operacional (atualiza√ß√£o semanal)",
                "Dashboard Estrat√©gico (atualiza√ß√£o mensal)"
            ],
            "alertas": [
                "ROI abaixo de 200% - Alerta cr√≠tico",
                "Taxa de convers√£o abaixo de 3% - Alerta m√©dio",
                "Custo de aquisi√ß√£o acima de R$ 800 - Alerta alto",
                "Churn rate acima de 10% - Alerta cr√≠tico"
            ],
            "relatorios": [
                "Relat√≥rio semanal de performance",
                "Relat√≥rio mensal de resultados",
                "Relat√≥rio trimestral estrat√©gico",
                "Relat√≥rio anual de crescimento"
            ],
            "otimizacoes": [
                "A/B testing cont√≠nuo em campanhas",
                "Otimiza√ß√£o de funil de vendas",
                "Melhoria cont√≠nua de processos",
                "An√°lise preditiva de tend√™ncias"
            ]
        }
    
    def _calculate_ultra_quality_score(self, analysis: Dict[str, Any]) -> float:
        """Calcula score de qualidade ultra-detalhado"""
        score = 0.0
        max_score = 100.0
        
        # Pontua√ß√£o por se√ß√µes implementadas (40 pontos)
        required_sections = [
            "avatar_ultra_detalhado", "escopo", "estrategia_palavras_chave",
            "insights_exclusivos_ultra", "plano_implementacao_completo", "metricas_sucesso_avancadas"
        ]
        
        for section in required_sections:
            if section in analysis and analysis[section]:
                score += 6.67  # 40/6 = 6.67 pontos por se√ß√£o
        
        # Pontua√ß√£o por profundidade de insights (30 pontos)
        insights = analysis.get("insights_exclusivos_ultra", [])
        if len(insights) >= 15:
            score += 30.0
        elif len(insights) >= 10:
            score += 20.0
        elif len(insights) >= 5:
            score += 10.0
        
        # Pontua√ß√£o por dados de pesquisa (30 pontos)
        if "pesquisa_web_detalhada" in analysis:
            score += 15.0
        if "inteligencia_mercado_ultra" in analysis:
            score += 15.0
        
        return min(score, max_score)
    
    def _calculate_completeness_score(self, analysis: Dict[str, Any]) -> float:
        """Calcula score de completude da an√°lise"""
        total_sections = 10  # Total de se√ß√µes principais
        completed_sections = 0
        
        sections_to_check = [
            "avatar_ultra_detalhado", "escopo", "estrategia_palavras_chave",
            "insights_exclusivos_ultra", "plano_implementacao_completo", 
            "metricas_sucesso_avancadas", "cronograma_365_dias", "sistema_monitoramento",
            "inteligencia_mercado_ultra", "analise_concorrencia_ultra"
        ]
        
        for section in sections_to_check:
            if section in analysis and analysis[section]:
                completed_sections += 1
        
        return (completed_sections / len(sections_to_check)) * 100.0
    
    def _generate_emergency_ultra_fallback(self, data: Dict[str, Any], error: str) -> Dict[str, Any]:
        """Gera an√°lise de emerg√™ncia ultra-b√°sica"""
        logger.error(f"Gerando an√°lise de emerg√™ncia devido a: {error}")
        
        basic_analysis = self._generate_basic_analysis(data)
        
        basic_analysis["insights_exclusivos_ultra"] = [
            "‚ö†Ô∏è An√°lise gerada em modo de emerg√™ncia",
            f"üîß Erro detectado: {error}",
            "üîÑ Recomenda-se executar nova an√°lise com dados completos",
            "üìä Sistema detectou necessidade de an√°lise mais profunda",
            "‚úÖ Dados b√°sicos de mercado foram preservados"
        ]
        
        basic_analysis["metadata_ultra_detalhado"] = {
            "processing_time_seconds": 0,
            "analysis_engine": "Emergency Fallback Ultra",
            "generated_at": datetime.utcnow().isoformat(),
            "quality_score": 25.0,
            "completeness_score": 15.0,
            "error": error,
            "recommendation": "Execute nova an√°lise com configura√ß√£o completa"
        }
        
        return basic_analysis

# Inst√¢ncia global do analisador ultra-robusto
ultra_analyzer = UltraRobustAnalyzer()

@analysis_bp.route('/analyze', methods=['POST'])
def analyze_market():
    """Endpoint principal para an√°lise ultra-robusta de mercado"""
    
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({
                'error': 'Dados n√£o fornecidos',
                'message': 'Envie os dados da an√°lise no corpo da requisi√ß√£o'
            }), 400
        
        # Valida√ß√£o b√°sica
        if not data.get('segmento'):
            return jsonify({
                'error': 'Segmento obrigat√≥rio',
                'message': 'O campo "segmento" √© obrigat√≥rio para a an√°lise'
            }), 400
        
        # Processa dados num√©ricos
        for field in ['preco', 'objetivo_receita', 'orcamento_marketing']:
            if field in data and data[field]:
                try:
                    data[f'{field}_float'] = float(str(data[field]).replace(',', '.'))
                except (ValueError, TypeError):
                    data[f'{field}_float'] = 0.0
        
        # Obt√©m session_id
        session_id = data.get('session_id') or session.get('session_id')
        
        logger.info(f"üöÄ Iniciando an√°lise ultra-robusta para: {data.get('segmento')}")
        
        # Executa an√°lise ultra-robusta
        result = ultra_analyzer.generate_ultra_comprehensive_analysis(data, session_id)
        
        # Salva no banco de dados
        if result and 'error' not in result:
            try:
                analysis_record = db_manager.create_analysis({
                    'segmento': data.get('segmento'),
                    'produto': data.get('produto'),
                    'preco': data.get('preco_float'),
                    'publico': data.get('publico'),
                    'concorrentes': data.get('concorrentes'),
                    'dados_adicionais': data.get('dados_adicionais'),
                    'objetivo_receita': data.get('objetivo_receita_float'),
                    'orcamento_marketing': data.get('orcamento_marketing_float'),
                    'prazo_lancamento': data.get('prazo_lancamento'),
                    'comprehensive_analysis': result
                })
                
                if analysis_record:
                    result['database_id'] = analysis_record['id']
                    logger.info(f"‚úÖ An√°lise salva no banco com ID: {analysis_record['id']}")
                
            except Exception as e:
                logger.error(f"‚ö†Ô∏è Erro ao salvar no banco: {str(e)}")
                # N√£o falha a an√°lise por erro de banco
        
        logger.info("üéâ An√°lise ultra-robusta conclu√≠da com sucesso!")
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"‚ùå Erro cr√≠tico na an√°lise: {str(e)}", exc_info=True)
        return jsonify({
            'error': 'Erro interno na an√°lise',
            'message': str(e),
            'timestamp': datetime.utcnow().isoformat()
        }), 500

@analysis_bp.route('/upload_attachment', methods=['POST'])
def upload_attachment():
    """Upload e processamento de anexos"""
    
    try:
        if 'file' not in request.files:
            return jsonify({
                'success': False,
                'error': 'Nenhum arquivo enviado'
            }), 400
        
        file = request.files['file']
        session_id = request.form.get('session_id', 'default_session')
        
        if file.filename == '':
            return jsonify({
                'success': False,
                'error': 'Nome de arquivo vazio'
            }), 400
        
        # Processa anexo
        result = attachment_service.process_attachment(file, session_id)
        
        if result.get('success'):
            logger.info(f"üìé Anexo processado: {file.filename}")
            return jsonify(result)
        else:
            return jsonify(result), 400
            
    except Exception as e:
        logger.error(f"‚ùå Erro no upload: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'Erro interno: {str(e)}'
        }), 500

@analysis_bp.route('/deep_search', methods=['POST'])
def perform_deep_search():
    """Executa busca profunda na internet"""
    
    try:
        data = request.get_json()
        
        if not data or not data.get('query'):
            return jsonify({
                'error': 'Query obrigat√≥ria',
                'message': 'Forne√ßa uma query para busca'
            }), 400
        
        query = data['query']
        context = data.get('context', {})
        
        logger.info(f"üîç Executando busca profunda: {query}")
        
        # Executa busca profunda
        result = deep_search_service.perform_deep_search(query, context)
        
        return jsonify({
            'query': query,
            'context': context,
            'result': result,
            'timestamp': datetime.utcnow().isoformat()
        })
        
    except Exception as e:
        logger.error(f"‚ùå Erro na busca profunda: {str(e)}")
        return jsonify({
            'error': 'Erro na busca profunda',
            'message': str(e)
        }), 500

@analysis_bp.route('/analyses', methods=['GET'])
def list_analyses():
    """Lista an√°lises salvas"""
    
    try:
        limit = min(int(request.args.get('limit', 20)), 100)
        offset = int(request.args.get('offset', 0))
        segmento = request.args.get('segmento')
        
        analyses = db_manager.list_analyses(limit, offset)
        
        # Filtra por segmento se especificado
        if segmento:
            analyses = [a for a in analyses if segmento.lower() in a.get('nicho', '').lower()]
        
        return jsonify({
            'analyses': analyses,
            'count': len(analyses),
            'limit': limit,
            'offset': offset
        })
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao listar an√°lises: {str(e)}")
        return jsonify({
            'error': 'Erro ao listar an√°lises',
            'message': str(e)
        }), 500

@analysis_bp.route('/analyses/<int:analysis_id>', methods=['GET'])
def get_analysis(analysis_id):
    """Obt√©m an√°lise espec√≠fica"""
    
    try:
        analysis = db_manager.get_analysis(analysis_id)
        
        if not analysis:
            return jsonify({
                'error': 'An√°lise n√£o encontrada',
                'message': f'An√°lise com ID {analysis_id} n√£o existe'
            }), 404
        
        return jsonify(analysis)
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao obter an√°lise {analysis_id}: {str(e)}")
        return jsonify({
            'error': 'Erro ao obter an√°lise',
            'message': str(e)
        }), 500

@analysis_bp.route('/stats', methods=['GET'])
def get_analysis_stats():
    """Obt√©m estat√≠sticas das an√°lises"""
    
    try:
        stats = db_manager.get_stats()
        return jsonify(stats)
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao obter estat√≠sticas: {str(e)}")
        return jsonify({
            'error': 'Erro ao obter estat√≠sticas',
            'message': str(e)
        }), 500